{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-316db3f4698f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import edward as ed\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import sys\n",
    "import gzip\n",
    "import pickle\n",
    "\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from edward.models import Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV0AAADhCAYAAABr92YNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x20580164240>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# DATA\n",
    "def load_mnist():\n",
    "    \"\"\"\n",
    "    Loads the MNIST handwritten digits dataset into three tuples training_data/\n",
    "\n",
    "    :return: Three tuples containing training data, validation data and test data\n",
    "    \"\"\"\n",
    "    f = gzip.open(r'C:\\Code\\neural-nets\\data\\mnist.pkl.gz')\n",
    "    training_data, validation_data, test_data = pickle.load(f, encoding='latin1')\n",
    "    f.close()\n",
    "    return training_data, validation_data, test_data\n",
    "\n",
    "training_data, validation_data, test_data = load_mnist()\n",
    "\n",
    "def show_grid_of_digits(data,nc,nr):\n",
    "    for i in range(nc*nr):\n",
    "        plt.subplot(nc,nr,i+1)\n",
    "        im = data[0][i].reshape(28,28)\n",
    "        plt.imshow(im,cmap='gray')\n",
    "        plt.gca().axes.get_xaxis().set_visible(False)\n",
    "        plt.gca().axes.get_yaxis().set_visible(False)\n",
    "        \n",
    "show_grid_of_digits(training_data,3,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def multilayer_perceptron(x, weights, biases):\n",
    "    \"\"\"\n",
    "    This function takes in the input placeholder, weights and biases and returns the output tensor of a network with\n",
    "    two hidden ReLU layers, and an output layer with linear activation.\n",
    "\n",
    "    :param tf.placeholder x: Placeholder for input\n",
    "    :param dict weights: Dictionary containing Variables describing weights of each layer\n",
    "    :param dict biases: Dictionary containing Variables describing biases of each layer\n",
    "    :return: The activations of the output layer\n",
    "    \"\"\"\n",
    "    # Hidden layer with RELU activation\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    # Hidden layer with RELU activation\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    # Output layer with linear activation\n",
    "    out_layer = tf.matmul(layer_2, weights['out']) + biases['out']\n",
    "    return out_layer\n",
    "\n",
    "n_input = 28**2\n",
    "n_hidden_1 = 10\n",
    "n_hidden_2 = 10\n",
    "n_classes = 10\n",
    "\n",
    "batch_size = 20\n",
    "\n",
    "# Priors on layers weight & bias\n",
    "weights = {\n",
    "    'h1': Normal(loc=tf.zeros([n_input, n_hidden_1]), scale=tf.ones([n_input, n_hidden_1])), \n",
    "    'h2': Normal(loc=tf.zeros([n_hidden_1, n_hidden_2]), scale=tf.ones([n_hidden_1, n_hidden_2])),\n",
    "    'out': Normal(loc=tf.zeros([n_hidden_2, n_classes]), scale=tf.ones([n_hidden_2, n_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'b1': Normal(loc=tf.zeros(n_hidden_1), scale=tf.ones(n_hidden_1)),# tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'b2': Normal(loc=tf.zeros(n_hidden_2), scale=tf.ones(n_hidden_2)),#tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'out': Normal(loc=tf.zeros(n_classes), scale=tf.ones(n_classes))#tf.Variable(tf.random_normal([n_classes]))\n",
    "}\n",
    "\n",
    "x = tf.placeholder(tf.float32, [batch_size, n_input])\n",
    "y = Normal(loc=multilayer_perceptron(x, weights, biases),scale=tf.ones(n_classes) * 0.1)  # constant noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# BACKWARD MODEL \n",
    "\n",
    "q_weights = {\n",
    "    'h1':  Normal(loc=tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n",
    "               scale=tf.nn.softplus(tf.Variable(tf.random_normal([n_input, n_hidden_1])))), \n",
    "    'h2': Normal(loc=tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "               scale=tf.nn.softplus(tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])))),\n",
    "    'out': Normal(loc=tf.Variable(tf.random_normal([n_hidden_2, n_classes])),\n",
    "               scale=tf.nn.softplus(tf.Variable(tf.random_normal([n_hidden_2, n_classes]))))\n",
    "}\n",
    "q_biases = {\n",
    "    'b1': Normal(loc=tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "               scale=tf.nn.softplus(tf.Variable(tf.random_normal([n_hidden_1])))),\n",
    "    'b2': Normal(loc=tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "               scale=tf.nn.softplus(tf.Variable(tf.random_normal([n_hidden_2])))),\n",
    "    'out': Normal(loc=tf.Variable(tf.random_normal([n_classes])),\n",
    "               scale=tf.nn.softplus(tf.Variable(tf.random_normal([n_classes]))))\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_x = training_data[0][:20]\n",
    "batch_y = to_categorical(training_data[1][:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [100%] ██████████████████████████████ Elapsed: 442s | Loss: 5028.379\n"
     ]
    }
   ],
   "source": [
    "# INFERENCE \n",
    "# this will take a couple of minutes\n",
    "\n",
    "latent_vars = {weights['h1']: q_weights['h1'], biases['b1']: q_biases['b1'],\n",
    "              weights['h2']: q_weights['h2'], biases['b2']: q_biases['b2'],\n",
    "              weights['out']: q_weights['out'], biases['out']: q_biases['out']}\n",
    "\n",
    "inference = ed.KLqp(latent_vars=latent_vars, \n",
    "                    data={x: batch_x, y:batch_y})\n",
    "\n",
    "inference.run(n_samples=10, n_iter=25000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.],\n",
       "       [ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 0, 4, 1, 9, 2, 1, 3, 1, 4, 3, 5, 3, 6, 1, 7, 2, 8, 6, 9], dtype=int64)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from edward.models import Categorical, Multinomial, Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Multinomial in module abc:\n",
      "\n",
      "class Multinomial(edward.models.random_variable.RandomVariable, tensorflow.contrib.distributions.python.ops.multinomial.Multinomial)\n",
      " |  Multinomial distribution.\n",
      " |  \n",
      " |  This Multinomial distribution is parameterized by `probs`, a (batch of)\n",
      " |  length-`k` `prob` (probability) vectors (`k > 1`) such that\n",
      " |  `tf.reduce_sum(probs, -1) = 1`, and a `total_count` number of trials, i.e.,\n",
      " |  the number of trials per draw from the Multinomial. It is defined over a\n",
      " |  (batch of) length-`k` vector `counts` such that\n",
      " |  `tf.reduce_sum(counts, -1) = total_count`. The Multinomial is identically the\n",
      " |  Binomial distribution when `k = 2`.\n",
      " |  \n",
      " |  #### Mathematical Details\n",
      " |  \n",
      " |  The Multinomial is a distribution over `k`-class counts, i.e., a length-`k`\n",
      " |  vector of non-negative integer `counts = n = [n_0, ..., n_{k-1}]`.\n",
      " |  \n",
      " |  The probability mass function (pmf) is,\n",
      " |  \n",
      " |  ```none\n",
      " |  pmf(n; pi, N) = prod_j (pi_j)**n_j / Z\n",
      " |  Z = (prod_j n_j!) / N!\n",
      " |  ```\n",
      " |  \n",
      " |  where:\n",
      " |  * `probs = pi = [pi_0, ..., pi_{k-1}]`, `pi_j > 0`, `sum_j pi_j = 1`,\n",
      " |  * `total_count = N`, `N` a positive integer,\n",
      " |  * `Z` is the normalization constant, and,\n",
      " |  * `N!` denotes `N` factorial.\n",
      " |  \n",
      " |  Distribution parameters are automatically broadcast in all functions; see\n",
      " |  examples for details.\n",
      " |  \n",
      " |  #### Examples\n",
      " |  \n",
      " |  Create a 3-class distribution, with the 3rd class is most likely to be drawn,\n",
      " |  using logits.\n",
      " |  \n",
      " |  ```python\n",
      " |  logits = [-50., -43, 0]\n",
      " |  dist = Multinomial(total_count=4., logits=logits)\n",
      " |  ```\n",
      " |  \n",
      " |  Create a 3-class distribution, with the 3rd class is most likely to be drawn.\n",
      " |  \n",
      " |  ```python\n",
      " |  p = [.2, .3, .5]\n",
      " |  dist = Multinomial(total_count=4., probs=p)\n",
      " |  ```\n",
      " |  \n",
      " |  The distribution functions can be evaluated on counts.\n",
      " |  \n",
      " |  ```python\n",
      " |  # counts same shape as p.\n",
      " |  counts = [1., 0, 3]\n",
      " |  dist.prob(counts)  # Shape []\n",
      " |  \n",
      " |  # p will be broadcast to [[.2, .3, .5], [.2, .3, .5]] to match counts.\n",
      " |  counts = [[1., 2, 1], [2, 2, 0]]\n",
      " |  dist.prob(counts)  # Shape [2]\n",
      " |  \n",
      " |  # p will be broadcast to shape [5, 7, 3] to match counts.\n",
      " |  counts = [[...]]  # Shape [5, 7, 3]\n",
      " |  dist.prob(counts)  # Shape [5, 7]\n",
      " |  ```\n",
      " |  \n",
      " |  Create a 2-batch of 3-class distributions.\n",
      " |  \n",
      " |  ```python\n",
      " |  p = [[.1, .2, .7], [.3, .3, .4]]  # Shape [2, 3]\n",
      " |  dist = Multinomial(total_count=[4., 5], probs=p)\n",
      " |  \n",
      " |  counts = [[2., 1, 1], [3, 1, 1]]\n",
      " |  dist.prob(counts)  # Shape [2]\n",
      " |  ```\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Multinomial\n",
      " |      edward.models.random_variable.RandomVariable\n",
      " |      tensorflow.contrib.distributions.python.ops.multinomial.Multinomial\n",
      " |      tensorflow.contrib.distributions.python.ops.distribution.Distribution\n",
      " |      tensorflow.contrib.distributions.python.ops.distribution._BaseDistribution\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  conjugate_log_prob = wrapped(self, val=None)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  support = 'onehot'\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from edward.models.random_variable.RandomVariable:\n",
      " |  \n",
      " |  __abs__(self)\n",
      " |  \n",
      " |  __add__(self, other)\n",
      " |  \n",
      " |  __and__(self, other)\n",
      " |  \n",
      " |  __bool__(self)\n",
      " |  \n",
      " |  __div__(self, other)\n",
      " |  \n",
      " |  __eq__(self, other)\n",
      " |      Return self==value.\n",
      " |  \n",
      " |  __floordiv__(self, other)\n",
      " |  \n",
      " |  __ge__(self, other)\n",
      " |      Return self>=value.\n",
      " |  \n",
      " |  __getitem__(self, key)\n",
      " |      Subset the tensor associated to the random variable, not the\n",
      " |      random variable itself.\n",
      " |  \n",
      " |  __gt__(self, other)\n",
      " |      Return self>value.\n",
      " |  \n",
      " |  __hash__(self)\n",
      " |      Return hash(self).\n",
      " |  \n",
      " |  __init__(self, *args, **kwargs)\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      sample_shape : tf.TensorShape, optional\n",
      " |        Shape of samples to draw from the random variable.\n",
      " |      value : tf.Tensor, optional\n",
      " |        Fixed tensor to associate with random variable. Must have shape\n",
      " |        ``sample_shape + batch_shape + event_shape``.\n",
      " |      collections : list, optional\n",
      " |        Optional list of graph collections keys. The random variable is\n",
      " |        added to these collections. Defaults to [\"random_variables\"].\n",
      " |      *args, **kwargs\n",
      " |        Passed into parent ``__init__``.\n",
      " |  \n",
      " |  __invert__(self)\n",
      " |  \n",
      " |  __iter__(self)\n",
      " |  \n",
      " |  __le__(self, other)\n",
      " |      Return self<=value.\n",
      " |  \n",
      " |  __lt__(self, other)\n",
      " |      Return self<value.\n",
      " |  \n",
      " |  __mod__(self, other)\n",
      " |  \n",
      " |  __mul__(self, other)\n",
      " |  \n",
      " |  __neg__(self)\n",
      " |  \n",
      " |  __nonzero__(self)\n",
      " |  \n",
      " |  __or__(self, other)\n",
      " |  \n",
      " |  __pow__(self, other)\n",
      " |  \n",
      " |  __radd__(self, other)\n",
      " |  \n",
      " |  __rand__(self, other)\n",
      " |  \n",
      " |  __rdiv__(self, other)\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __rfloordiv__(self, other)\n",
      " |  \n",
      " |  __rmod__(self, other)\n",
      " |  \n",
      " |  __rmul__(self, other)\n",
      " |  \n",
      " |  __ror__(self, other)\n",
      " |  \n",
      " |  __rpow__(self, other)\n",
      " |  \n",
      " |  __rsub__(self, other)\n",
      " |  \n",
      " |  __rtruediv__ = __rdiv__(self, other)\n",
      " |  \n",
      " |  __rxor__(self, other)\n",
      " |  \n",
      " |  __str__(self)\n",
      " |      Return str(self).\n",
      " |  \n",
      " |  __sub__(self, other)\n",
      " |  \n",
      " |  __truediv__ = __div__(self, other)\n",
      " |  \n",
      " |  __xor__(self, other)\n",
      " |  \n",
      " |  eval(self, session=None, feed_dict=None)\n",
      " |      In a session, computes and returns the value of this random variable.\n",
      " |      \n",
      " |      This is not a graph construction method, it does not add ops to the graph.\n",
      " |      \n",
      " |      This convenience method requires a session where the graph\n",
      " |      containing this variable has been launched. If no session is\n",
      " |      passed, the default session is used.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      session : tf.BaseSession, optional\n",
      " |        The ``tf.Session`` to use to evaluate this random variable. If\n",
      " |        none, the default session is used.\n",
      " |      feed_dict : dict, optional\n",
      " |        A dictionary that maps ``tf.Tensor`` objects to feed values. See\n",
      " |        ``tf.Session.run()`` for a description of the valid feed values.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> x = Normal(0.0, 1.0)\n",
      " |      >>> with tf.Session() as sess:\n",
      " |      >>>   # Usage passing the session explicitly.\n",
      " |      >>>   print(x.eval(sess))\n",
      " |      >>>   # Usage with the default session.  The 'with' block\n",
      " |      >>>   # above makes 'sess' the default session.\n",
      " |      >>>   print(x.eval())\n",
      " |  \n",
      " |  get_ancestors(self, collection=None)\n",
      " |      Get ancestor random variables.\n",
      " |  \n",
      " |  get_blanket(self, collection=None)\n",
      " |      Get the random variable's Markov blanket.\n",
      " |  \n",
      " |  get_children(self, collection=None)\n",
      " |      Get child random variables.\n",
      " |  \n",
      " |  get_descendants(self, collection=None)\n",
      " |      Get descendant random variables.\n",
      " |  \n",
      " |  get_parents(self, collection=None)\n",
      " |      Get parent random variables.\n",
      " |  \n",
      " |  get_shape(self)\n",
      " |      Get shape of random variable.\n",
      " |  \n",
      " |  get_siblings(self, collection=None)\n",
      " |      Get sibling random variables.\n",
      " |  \n",
      " |  get_variables(self, collection=None)\n",
      " |      Get TensorFlow variables that the random variable depends on.\n",
      " |  \n",
      " |  value(self)\n",
      " |      Get tensor that the random variable corresponds to.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from edward.models.random_variable.RandomVariable:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  sample_shape\n",
      " |      Sample shape of random variable.\n",
      " |  \n",
      " |  shape\n",
      " |      Shape of random variable.\n",
      " |  \n",
      " |  unique_name\n",
      " |      Name of random variable with its unique scoping name. Use\n",
      " |      ``name`` to just get the name of the random variable.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from tensorflow.contrib.distributions.python.ops.multinomial.Multinomial:\n",
      " |  \n",
      " |  log_prob(self, value, name='log_prob')\n",
      " |      Log probability density/mass function.\n",
      " |      \n",
      " |      \n",
      " |      Additional documentation from `Multinomial`:\n",
      " |      \n",
      " |      For each batch of counts, `value = [n_0, ...\n",
      " |      ,n_{k-1}]`, `P[value]` is the probability that after sampling `self.total_count`\n",
      " |      draws from this Multinomial distribution, the number of draws falling in class\n",
      " |      `j` is `n_j`. Since this definition is [exchangeable](\n",
      " |      https://en.wikipedia.org/wiki/Exchangeable_random_variables); different\n",
      " |      sequences have the same counts so the probability includes a combinatorial\n",
      " |      coefficient.\n",
      " |      \n",
      " |      Note: `value` must be a non-negative tensor with dtype `self.dtype`, have no\n",
      " |      fractional components, and such that\n",
      " |      `tf.reduce_sum(value, -1) = self.total_count`. Its shape must be broadcastable\n",
      " |      with `self.probs` and `self.total_count`.\n",
      " |      \n",
      " |      Args:\n",
      " |        value: `float` or `double` `Tensor`.\n",
      " |        name: The name to give this op.\n",
      " |      \n",
      " |      Returns:\n",
      " |        log_prob: a `Tensor` of shape `sample_shape(x) + self.batch_shape` with\n",
      " |          values of type `self.dtype`.\n",
      " |  \n",
      " |  prob(self, value, name='prob')\n",
      " |      Probability density/mass function.\n",
      " |      \n",
      " |      \n",
      " |      Additional documentation from `Multinomial`:\n",
      " |      \n",
      " |      For each batch of counts, `value = [n_0, ...\n",
      " |      ,n_{k-1}]`, `P[value]` is the probability that after sampling `self.total_count`\n",
      " |      draws from this Multinomial distribution, the number of draws falling in class\n",
      " |      `j` is `n_j`. Since this definition is [exchangeable](\n",
      " |      https://en.wikipedia.org/wiki/Exchangeable_random_variables); different\n",
      " |      sequences have the same counts so the probability includes a combinatorial\n",
      " |      coefficient.\n",
      " |      \n",
      " |      Note: `value` must be a non-negative tensor with dtype `self.dtype`, have no\n",
      " |      fractional components, and such that\n",
      " |      `tf.reduce_sum(value, -1) = self.total_count`. Its shape must be broadcastable\n",
      " |      with `self.probs` and `self.total_count`.\n",
      " |      \n",
      " |      Args:\n",
      " |        value: `float` or `double` `Tensor`.\n",
      " |        name: The name to give this op.\n",
      " |      \n",
      " |      Returns:\n",
      " |        prob: a `Tensor` of shape `sample_shape(x) + self.batch_shape` with\n",
      " |          values of type `self.dtype`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from tensorflow.contrib.distributions.python.ops.multinomial.Multinomial:\n",
      " |  \n",
      " |  logits\n",
      " |      Vector of coordinatewise logits.\n",
      " |  \n",
      " |  probs\n",
      " |      Probability of of drawing a `1` in that coordinate.\n",
      " |  \n",
      " |  total_count\n",
      " |      Number of trials used to construct a sample.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from tensorflow.contrib.distributions.python.ops.distribution.Distribution:\n",
      " |  \n",
      " |  batch_shape_tensor(self, name='batch_shape_tensor')\n",
      " |      Shape of a single sample from a single event index as a 1-D `Tensor`.\n",
      " |      \n",
      " |      The batch dimensions are indexes into independent, non-identical\n",
      " |      parameterizations of this distribution.\n",
      " |      \n",
      " |      Args:\n",
      " |        name: name to give to the op\n",
      " |      \n",
      " |      Returns:\n",
      " |        batch_shape: `Tensor`.\n",
      " |  \n",
      " |  cdf(self, value, name='cdf')\n",
      " |      Cumulative distribution function.\n",
      " |      \n",
      " |      Given random variable `X`, the cumulative distribution function `cdf` is:\n",
      " |      \n",
      " |      ```none\n",
      " |      cdf(x) := P[X <= x]\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |        value: `float` or `double` `Tensor`.\n",
      " |        name: The name to give this op.\n",
      " |      \n",
      " |      Returns:\n",
      " |        cdf: a `Tensor` of shape `sample_shape(x) + self.batch_shape` with\n",
      " |          values of type `self.dtype`.\n",
      " |  \n",
      " |  copy(self, **override_parameters_kwargs)\n",
      " |      Creates a deep copy of the distribution.\n",
      " |      \n",
      " |      Note: the copy distribution may continue to depend on the original\n",
      " |      initialization arguments.\n",
      " |      \n",
      " |      Args:\n",
      " |        **override_parameters_kwargs: String/value dictionary of initialization\n",
      " |          arguments to override with new values.\n",
      " |      \n",
      " |      Returns:\n",
      " |        distribution: A new instance of `type(self)` initialized from the union\n",
      " |          of self.parameters and override_parameters_kwargs, i.e.,\n",
      " |          `dict(self.parameters, **override_parameters_kwargs)`.\n",
      " |  \n",
      " |  covariance(self, name='covariance')\n",
      " |      Covariance.\n",
      " |      \n",
      " |      Covariance is (possibly) defined only for non-scalar-event distributions.\n",
      " |      \n",
      " |      For example, for a length-`k`, vector-valued distribution, it is calculated\n",
      " |      as,\n",
      " |      \n",
      " |      ```none\n",
      " |      Cov[i, j] = Covariance(X_i, X_j) = E[(X_i - E[X_i]) (X_j - E[X_j])]\n",
      " |      ```\n",
      " |      \n",
      " |      where `Cov` is a (batch of) `k x k` matrix, `0 <= (i, j) < k`, and `E`\n",
      " |      denotes expectation.\n",
      " |      \n",
      " |      Alternatively, for non-vector, multivariate distributions (e.g.,\n",
      " |      matrix-valued, Wishart), `Covariance` shall return a (batch of) matrices\n",
      " |      under some vectorization of the events, i.e.,\n",
      " |      \n",
      " |      ```none\n",
      " |      Cov[i, j] = Covariance(Vec(X)_i, Vec(X)_j) = [as above]\n",
      " |      ```\n",
      " |      \n",
      " |      where `Cov` is a (batch of) `k' x k'` matrices,\n",
      " |      `0 <= (i, j) < k' = reduce_prod(event_shape)`, and `Vec` is some function\n",
      " |      mapping indices of this distribution's event dimensions to indices of a\n",
      " |      length-`k'` vector.\n",
      " |      \n",
      " |      Args:\n",
      " |        name: The name to give this op.\n",
      " |      \n",
      " |      Returns:\n",
      " |        covariance: Floating-point `Tensor` with shape `[B1, ..., Bn, k', k']`\n",
      " |          where the first `n` dimensions are batch coordinates and\n",
      " |          `k' = reduce_prod(self.event_shape)`.\n",
      " |  \n",
      " |  entropy(self, name='entropy')\n",
      " |      Shannon entropy in nats.\n",
      " |  \n",
      " |  event_shape_tensor(self, name='event_shape_tensor')\n",
      " |      Shape of a single sample from a single batch as a 1-D int32 `Tensor`.\n",
      " |      \n",
      " |      Args:\n",
      " |        name: name to give to the op\n",
      " |      \n",
      " |      Returns:\n",
      " |        event_shape: `Tensor`.\n",
      " |  \n",
      " |  is_scalar_batch(self, name='is_scalar_batch')\n",
      " |      Indicates that `batch_shape == []`.\n",
      " |      \n",
      " |      Args:\n",
      " |        name: The name to give this op.\n",
      " |      \n",
      " |      Returns:\n",
      " |        is_scalar_batch: `bool` scalar `Tensor`.\n",
      " |  \n",
      " |  is_scalar_event(self, name='is_scalar_event')\n",
      " |      Indicates that `event_shape == []`.\n",
      " |      \n",
      " |      Args:\n",
      " |        name: The name to give this op.\n",
      " |      \n",
      " |      Returns:\n",
      " |        is_scalar_event: `bool` scalar `Tensor`.\n",
      " |  \n",
      " |  log_cdf(self, value, name='log_cdf')\n",
      " |      Log cumulative distribution function.\n",
      " |      \n",
      " |      Given random variable `X`, the cumulative distribution function `cdf` is:\n",
      " |      \n",
      " |      ```none\n",
      " |      log_cdf(x) := Log[ P[X <= x] ]\n",
      " |      ```\n",
      " |      \n",
      " |      Often, a numerical approximation can be used for `log_cdf(x)` that yields\n",
      " |      a more accurate answer than simply taking the logarithm of the `cdf` when\n",
      " |      `x << -1`.\n",
      " |      \n",
      " |      Args:\n",
      " |        value: `float` or `double` `Tensor`.\n",
      " |        name: The name to give this op.\n",
      " |      \n",
      " |      Returns:\n",
      " |        logcdf: a `Tensor` of shape `sample_shape(x) + self.batch_shape` with\n",
      " |          values of type `self.dtype`.\n",
      " |  \n",
      " |  log_survival_function(self, value, name='log_survival_function')\n",
      " |      Log survival function.\n",
      " |      \n",
      " |      Given random variable `X`, the survival function is defined:\n",
      " |      \n",
      " |      ```none\n",
      " |      log_survival_function(x) = Log[ P[X > x] ]\n",
      " |                               = Log[ 1 - P[X <= x] ]\n",
      " |                               = Log[ 1 - cdf(x) ]\n",
      " |      ```\n",
      " |      \n",
      " |      Typically, different numerical approximations can be used for the log\n",
      " |      survival function, which are more accurate than `1 - cdf(x)` when `x >> 1`.\n",
      " |      \n",
      " |      Args:\n",
      " |        value: `float` or `double` `Tensor`.\n",
      " |        name: The name to give this op.\n",
      " |      \n",
      " |      Returns:\n",
      " |        `Tensor` of shape `sample_shape(x) + self.batch_shape` with values of type\n",
      " |          `self.dtype`.\n",
      " |  \n",
      " |  mean(self, name='mean')\n",
      " |      Mean.\n",
      " |  \n",
      " |  mode(self, name='mode')\n",
      " |      Mode.\n",
      " |  \n",
      " |  sample(self, sample_shape=(), seed=None, name='sample')\n",
      " |      Generate samples of the specified shape.\n",
      " |      \n",
      " |      Note that a call to `sample()` without arguments will generate a single\n",
      " |      sample.\n",
      " |      \n",
      " |      Args:\n",
      " |        sample_shape: 0D or 1D `int32` `Tensor`. Shape of the generated samples.\n",
      " |        seed: Python integer seed for RNG\n",
      " |        name: name to give to the op.\n",
      " |      \n",
      " |      Returns:\n",
      " |        samples: a `Tensor` with prepended dimensions `sample_shape`.\n",
      " |  \n",
      " |  stddev(self, name='stddev')\n",
      " |      Standard deviation.\n",
      " |      \n",
      " |      Standard deviation is defined as,\n",
      " |      \n",
      " |      ```none\n",
      " |      stddev = E[(X - E[X])**2]**0.5\n",
      " |      ```\n",
      " |      \n",
      " |      where `X` is the random variable associated with this distribution, `E`\n",
      " |      denotes expectation, and `stddev.shape = batch_shape + event_shape`.\n",
      " |      \n",
      " |      Args:\n",
      " |        name: The name to give this op.\n",
      " |      \n",
      " |      Returns:\n",
      " |        stddev: Floating-point `Tensor` with shape identical to\n",
      " |          `batch_shape + event_shape`, i.e., the same shape as `self.mean()`.\n",
      " |  \n",
      " |  survival_function(self, value, name='survival_function')\n",
      " |      Survival function.\n",
      " |      \n",
      " |      Given random variable `X`, the survival function is defined:\n",
      " |      \n",
      " |      ```none\n",
      " |      survival_function(x) = P[X > x]\n",
      " |                           = 1 - P[X <= x]\n",
      " |                           = 1 - cdf(x).\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |        value: `float` or `double` `Tensor`.\n",
      " |        name: The name to give this op.\n",
      " |      \n",
      " |      Returns:\n",
      " |        `Tensor` of shape `sample_shape(x) + self.batch_shape` with values of type\n",
      " |          `self.dtype`.\n",
      " |  \n",
      " |  variance(self, name='variance')\n",
      " |      Variance.\n",
      " |      \n",
      " |      Variance is defined as,\n",
      " |      \n",
      " |      ```none\n",
      " |      Var = E[(X - E[X])**2]\n",
      " |      ```\n",
      " |      \n",
      " |      where `X` is the random variable associated with this distribution, `E`\n",
      " |      denotes expectation, and `Var.shape = batch_shape + event_shape`.\n",
      " |      \n",
      " |      Args:\n",
      " |        name: The name to give this op.\n",
      " |      \n",
      " |      Returns:\n",
      " |        variance: Floating-point `Tensor` with shape identical to\n",
      " |          `batch_shape + event_shape`, i.e., the same shape as `self.mean()`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from tensorflow.contrib.distributions.python.ops.distribution.Distribution:\n",
      " |  \n",
      " |  param_shapes(sample_shape, name='DistributionParamShapes') from tensorflow.contrib.distributions.python.ops.distribution._DistributionMeta\n",
      " |      Shapes of parameters given the desired shape of a call to `sample()`.\n",
      " |      \n",
      " |      This is a class method that describes what key/value arguments are required\n",
      " |      to instantiate the given `Distribution` so that a particular shape is\n",
      " |      returned for that instance's call to `sample()`.\n",
      " |      \n",
      " |      Subclasses should override class method `_param_shapes`.\n",
      " |      \n",
      " |      Args:\n",
      " |        sample_shape: `Tensor` or python list/tuple. Desired shape of a call to\n",
      " |          `sample()`.\n",
      " |        name: name to prepend ops with.\n",
      " |      \n",
      " |      Returns:\n",
      " |        `dict` of parameter name to `Tensor` shapes.\n",
      " |  \n",
      " |  param_static_shapes(sample_shape) from tensorflow.contrib.distributions.python.ops.distribution._DistributionMeta\n",
      " |      param_shapes with static (i.e. `TensorShape`) shapes.\n",
      " |      \n",
      " |      This is a class method that describes what key/value arguments are required\n",
      " |      to instantiate the given `Distribution` so that a particular shape is\n",
      " |      returned for that instance's call to `sample()`. Assumes that the sample's\n",
      " |      shape is known statically.\n",
      " |      \n",
      " |      Subclasses should override class method `_param_shapes` to return\n",
      " |      constant-valued tensors when constant values are fed.\n",
      " |      \n",
      " |      Args:\n",
      " |        sample_shape: `TensorShape` or python list/tuple. Desired shape of a call\n",
      " |          to `sample()`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        `dict` of parameter name to `TensorShape`.\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: if `sample_shape` is a `TensorShape` and is not fully defined.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from tensorflow.contrib.distributions.python.ops.distribution.Distribution:\n",
      " |  \n",
      " |  allow_nan_stats\n",
      " |      Python `bool` describing behavior when a stat is undefined.\n",
      " |      \n",
      " |      Stats return +/- infinity when it makes sense. E.g., the variance of a\n",
      " |      Cauchy distribution is infinity. However, sometimes the statistic is\n",
      " |      undefined, e.g., if a distribution's pdf does not achieve a maximum within\n",
      " |      the support of the distribution, the mode is undefined. If the mean is\n",
      " |      undefined, then by definition the variance is undefined. E.g. the mean for\n",
      " |      Student's T for df = 1 is undefined (no clear way to say it is either + or -\n",
      " |      infinity), so the variance = E[(X - mean)**2] is also undefined.\n",
      " |      \n",
      " |      Returns:\n",
      " |        allow_nan_stats: Python `bool`.\n",
      " |  \n",
      " |  batch_shape\n",
      " |      Shape of a single sample from a single event index as a `TensorShape`.\n",
      " |      \n",
      " |      May be partially defined or unknown.\n",
      " |      \n",
      " |      The batch dimensions are indexes into independent, non-identical\n",
      " |      parameterizations of this distribution.\n",
      " |      \n",
      " |      Returns:\n",
      " |        batch_shape: `TensorShape`, possibly unknown.\n",
      " |  \n",
      " |  dtype\n",
      " |      The `DType` of `Tensor`s handled by this `Distribution`.\n",
      " |  \n",
      " |  event_shape\n",
      " |      Shape of a single sample from a single batch as a `TensorShape`.\n",
      " |      \n",
      " |      May be partially defined or unknown.\n",
      " |      \n",
      " |      Returns:\n",
      " |        event_shape: `TensorShape`, possibly unknown.\n",
      " |  \n",
      " |  name\n",
      " |      Name prepended to all ops created by this `Distribution`.\n",
      " |  \n",
      " |  parameters\n",
      " |      Dictionary of parameters used to instantiate this `Distribution`.\n",
      " |  \n",
      " |  reparameterization_type\n",
      " |      Describes how samples from the distribution are reparameterized.\n",
      " |      \n",
      " |      Currently this is one of the static instances\n",
      " |      `distributions.FULLY_REPARAMETERIZED`\n",
      " |      or `distributions.NOT_REPARAMETERIZED`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        An instance of `ReparameterizationType`.\n",
      " |  \n",
      " |  validate_args\n",
      " |      Python `bool` indicating possibly expensive checks are enabled.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(Multinomial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logits = [-50., -43, 0]\n",
    "dist = Multinomial(total_count=4., logits=logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "H=20\n",
    "K=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_0 = Normal(loc=tf.zeros([D, H]), scale=tf.ones([D, H]))\n",
    "W_1 = Normal(loc=tf.zeros([H, K]), scale=tf.ones([H, K]))\n",
    "b_0 = Normal(loc=tf.zeros(H), scale=tf.ones(H))\n",
    "b_1 = Normal(loc=tf.zeros(K), scale=tf.ones(K))\n",
    "\n",
    "def neural_network(x):\n",
    "    h = tf.nn.tanh(tf.matmul(x, W_0) + b_0)\n",
    "    h = tf.matmul(h, W_1) + b_1\n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "M = 1\n",
    "D = 28**2\n",
    "x = tf.placeholder(tf.float32, [M, D])\n",
    "y = Multinomial(total_count=4., logits=neural_network(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ed.RandomVariable 'Multinomial_8/' shape=(1, 10) dtype=float32>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 1 required positional argument: 'total_count'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-1a09f569111d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMultinomial\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mneural_network\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\programdata\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\edward\\models\\random_variable.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    101\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_kwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'collections'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcollections\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 103\u001b[1;33m     \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mRandomVariable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sample_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensorShape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'total_count'"
     ]
    }
   ],
   "source": [
    "y = Multinomial(logits=neural_network(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Multinomial in module abc:\n",
      "\n",
      "class Multinomial(edward.models.random_variable.RandomVariable, tensorflow.contrib.distributions.python.ops.multinomial.Multinomial)\n",
      " |  Multinomial distribution.\n",
      " |  \n",
      " |  This Multinomial distribution is parameterized by `probs`, a (batch of)\n",
      " |  length-`k` `prob` (probability) vectors (`k > 1`) such that\n",
      " |  `tf.reduce_sum(probs, -1) = 1`, and a `total_count` number of trials, i.e.,\n",
      " |  the number of trials per draw from the Multinomial. It is defined over a\n",
      " |  (batch of) length-`k` vector `counts` such that\n",
      " |  `tf.reduce_sum(counts, -1) = total_count`. The Multinomial is identically the\n",
      " |  Binomial distribution when `k = 2`.\n",
      " |  \n",
      " |  #### Mathematical Details\n",
      " |  \n",
      " |  The Multinomial is a distribution over `k`-class counts, i.e., a length-`k`\n",
      " |  vector of non-negative integer `counts = n = [n_0, ..., n_{k-1}]`.\n",
      " |  \n",
      " |  The probability mass function (pmf) is,\n",
      " |  \n",
      " |  ```none\n",
      " |  pmf(n; pi, N) = prod_j (pi_j)**n_j / Z\n",
      " |  Z = (prod_j n_j!) / N!\n",
      " |  ```\n",
      " |  \n",
      " |  where:\n",
      " |  * `probs = pi = [pi_0, ..., pi_{k-1}]`, `pi_j > 0`, `sum_j pi_j = 1`,\n",
      " |  * `total_count = N`, `N` a positive integer,\n",
      " |  * `Z` is the normalization constant, and,\n",
      " |  * `N!` denotes `N` factorial.\n",
      " |  \n",
      " |  Distribution parameters are automatically broadcast in all functions; see\n",
      " |  examples for details.\n",
      " |  \n",
      " |  #### Examples\n",
      " |  \n",
      " |  Create a 3-class distribution, with the 3rd class is most likely to be drawn,\n",
      " |  using logits.\n",
      " |  \n",
      " |  ```python\n",
      " |  logits = [-50., -43, 0]\n",
      " |  dist = Multinomial(total_count=4., logits=logits)\n",
      " |  ```\n",
      " |  \n",
      " |  Create a 3-class distribution, with the 3rd class is most likely to be drawn.\n",
      " |  \n",
      " |  ```python\n",
      " |  p = [.2, .3, .5]\n",
      " |  dist = Multinomial(total_count=4., probs=p)\n",
      " |  ```\n",
      " |  \n",
      " |  The distribution functions can be evaluated on counts.\n",
      " |  \n",
      " |  ```python\n",
      " |  # counts same shape as p.\n",
      " |  counts = [1., 0, 3]\n",
      " |  dist.prob(counts)  # Shape []\n",
      " |  \n",
      " |  # p will be broadcast to [[.2, .3, .5], [.2, .3, .5]] to match counts.\n",
      " |  counts = [[1., 2, 1], [2, 2, 0]]\n",
      " |  dist.prob(counts)  # Shape [2]\n",
      " |  \n",
      " |  # p will be broadcast to shape [5, 7, 3] to match counts.\n",
      " |  counts = [[...]]  # Shape [5, 7, 3]\n",
      " |  dist.prob(counts)  # Shape [5, 7]\n",
      " |  ```\n",
      " |  \n",
      " |  Create a 2-batch of 3-class distributions.\n",
      " |  \n",
      " |  ```python\n",
      " |  p = [[.1, .2, .7], [.3, .3, .4]]  # Shape [2, 3]\n",
      " |  dist = Multinomial(total_count=[4., 5], probs=p)\n",
      " |  \n",
      " |  counts = [[2., 1, 1], [3, 1, 1]]\n",
      " |  dist.prob(counts)  # Shape [2]\n",
      " |  ```\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Multinomial\n",
      " |      edward.models.random_variable.RandomVariable\n",
      " |      tensorflow.contrib.distributions.python.ops.multinomial.Multinomial\n",
      " |      tensorflow.contrib.distributions.python.ops.distribution.Distribution\n",
      " |      tensorflow.contrib.distributions.python.ops.distribution._BaseDistribution\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  conjugate_log_prob = wrapped(self, val=None)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  support = 'onehot'\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from edward.models.random_variable.RandomVariable:\n",
      " |  \n",
      " |  __abs__(self)\n",
      " |  \n",
      " |  __add__(self, other)\n",
      " |  \n",
      " |  __and__(self, other)\n",
      " |  \n",
      " |  __bool__(self)\n",
      " |  \n",
      " |  __div__(self, other)\n",
      " |  \n",
      " |  __eq__(self, other)\n",
      " |      Return self==value.\n",
      " |  \n",
      " |  __floordiv__(self, other)\n",
      " |  \n",
      " |  __ge__(self, other)\n",
      " |      Return self>=value.\n",
      " |  \n",
      " |  __getitem__(self, key)\n",
      " |      Subset the tensor associated to the random variable, not the\n",
      " |      random variable itself.\n",
      " |  \n",
      " |  __gt__(self, other)\n",
      " |      Return self>value.\n",
      " |  \n",
      " |  __hash__(self)\n",
      " |      Return hash(self).\n",
      " |  \n",
      " |  __init__(self, *args, **kwargs)\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      sample_shape : tf.TensorShape, optional\n",
      " |        Shape of samples to draw from the random variable.\n",
      " |      value : tf.Tensor, optional\n",
      " |        Fixed tensor to associate with random variable. Must have shape\n",
      " |        ``sample_shape + batch_shape + event_shape``.\n",
      " |      collections : list, optional\n",
      " |        Optional list of graph collections keys. The random variable is\n",
      " |        added to these collections. Defaults to [\"random_variables\"].\n",
      " |      *args, **kwargs\n",
      " |        Passed into parent ``__init__``.\n",
      " |  \n",
      " |  __invert__(self)\n",
      " |  \n",
      " |  __iter__(self)\n",
      " |  \n",
      " |  __le__(self, other)\n",
      " |      Return self<=value.\n",
      " |  \n",
      " |  __lt__(self, other)\n",
      " |      Return self<value.\n",
      " |  \n",
      " |  __mod__(self, other)\n",
      " |  \n",
      " |  __mul__(self, other)\n",
      " |  \n",
      " |  __neg__(self)\n",
      " |  \n",
      " |  __nonzero__(self)\n",
      " |  \n",
      " |  __or__(self, other)\n",
      " |  \n",
      " |  __pow__(self, other)\n",
      " |  \n",
      " |  __radd__(self, other)\n",
      " |  \n",
      " |  __rand__(self, other)\n",
      " |  \n",
      " |  __rdiv__(self, other)\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __rfloordiv__(self, other)\n",
      " |  \n",
      " |  __rmod__(self, other)\n",
      " |  \n",
      " |  __rmul__(self, other)\n",
      " |  \n",
      " |  __ror__(self, other)\n",
      " |  \n",
      " |  __rpow__(self, other)\n",
      " |  \n",
      " |  __rsub__(self, other)\n",
      " |  \n",
      " |  __rtruediv__ = __rdiv__(self, other)\n",
      " |  \n",
      " |  __rxor__(self, other)\n",
      " |  \n",
      " |  __str__(self)\n",
      " |      Return str(self).\n",
      " |  \n",
      " |  __sub__(self, other)\n",
      " |  \n",
      " |  __truediv__ = __div__(self, other)\n",
      " |  \n",
      " |  __xor__(self, other)\n",
      " |  \n",
      " |  eval(self, session=None, feed_dict=None)\n",
      " |      In a session, computes and returns the value of this random variable.\n",
      " |      \n",
      " |      This is not a graph construction method, it does not add ops to the graph.\n",
      " |      \n",
      " |      This convenience method requires a session where the graph\n",
      " |      containing this variable has been launched. If no session is\n",
      " |      passed, the default session is used.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      session : tf.BaseSession, optional\n",
      " |        The ``tf.Session`` to use to evaluate this random variable. If\n",
      " |        none, the default session is used.\n",
      " |      feed_dict : dict, optional\n",
      " |        A dictionary that maps ``tf.Tensor`` objects to feed values. See\n",
      " |        ``tf.Session.run()`` for a description of the valid feed values.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> x = Normal(0.0, 1.0)\n",
      " |      >>> with tf.Session() as sess:\n",
      " |      >>>   # Usage passing the session explicitly.\n",
      " |      >>>   print(x.eval(sess))\n",
      " |      >>>   # Usage with the default session.  The 'with' block\n",
      " |      >>>   # above makes 'sess' the default session.\n",
      " |      >>>   print(x.eval())\n",
      " |  \n",
      " |  get_ancestors(self, collection=None)\n",
      " |      Get ancestor random variables.\n",
      " |  \n",
      " |  get_blanket(self, collection=None)\n",
      " |      Get the random variable's Markov blanket.\n",
      " |  \n",
      " |  get_children(self, collection=None)\n",
      " |      Get child random variables.\n",
      " |  \n",
      " |  get_descendants(self, collection=None)\n",
      " |      Get descendant random variables.\n",
      " |  \n",
      " |  get_parents(self, collection=None)\n",
      " |      Get parent random variables.\n",
      " |  \n",
      " |  get_shape(self)\n",
      " |      Get shape of random variable.\n",
      " |  \n",
      " |  get_siblings(self, collection=None)\n",
      " |      Get sibling random variables.\n",
      " |  \n",
      " |  get_variables(self, collection=None)\n",
      " |      Get TensorFlow variables that the random variable depends on.\n",
      " |  \n",
      " |  value(self)\n",
      " |      Get tensor that the random variable corresponds to.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from edward.models.random_variable.RandomVariable:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  sample_shape\n",
      " |      Sample shape of random variable.\n",
      " |  \n",
      " |  shape\n",
      " |      Shape of random variable.\n",
      " |  \n",
      " |  unique_name\n",
      " |      Name of random variable with its unique scoping name. Use\n",
      " |      ``name`` to just get the name of the random variable.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from tensorflow.contrib.distributions.python.ops.multinomial.Multinomial:\n",
      " |  \n",
      " |  log_prob(self, value, name='log_prob')\n",
      " |      Log probability density/mass function.\n",
      " |      \n",
      " |      \n",
      " |      Additional documentation from `Multinomial`:\n",
      " |      \n",
      " |      For each batch of counts, `value = [n_0, ...\n",
      " |      ,n_{k-1}]`, `P[value]` is the probability that after sampling `self.total_count`\n",
      " |      draws from this Multinomial distribution, the number of draws falling in class\n",
      " |      `j` is `n_j`. Since this definition is [exchangeable](\n",
      " |      https://en.wikipedia.org/wiki/Exchangeable_random_variables); different\n",
      " |      sequences have the same counts so the probability includes a combinatorial\n",
      " |      coefficient.\n",
      " |      \n",
      " |      Note: `value` must be a non-negative tensor with dtype `self.dtype`, have no\n",
      " |      fractional components, and such that\n",
      " |      `tf.reduce_sum(value, -1) = self.total_count`. Its shape must be broadcastable\n",
      " |      with `self.probs` and `self.total_count`.\n",
      " |      \n",
      " |      Args:\n",
      " |        value: `float` or `double` `Tensor`.\n",
      " |        name: The name to give this op.\n",
      " |      \n",
      " |      Returns:\n",
      " |        log_prob: a `Tensor` of shape `sample_shape(x) + self.batch_shape` with\n",
      " |          values of type `self.dtype`.\n",
      " |  \n",
      " |  prob(self, value, name='prob')\n",
      " |      Probability density/mass function.\n",
      " |      \n",
      " |      \n",
      " |      Additional documentation from `Multinomial`:\n",
      " |      \n",
      " |      For each batch of counts, `value = [n_0, ...\n",
      " |      ,n_{k-1}]`, `P[value]` is the probability that after sampling `self.total_count`\n",
      " |      draws from this Multinomial distribution, the number of draws falling in class\n",
      " |      `j` is `n_j`. Since this definition is [exchangeable](\n",
      " |      https://en.wikipedia.org/wiki/Exchangeable_random_variables); different\n",
      " |      sequences have the same counts so the probability includes a combinatorial\n",
      " |      coefficient.\n",
      " |      \n",
      " |      Note: `value` must be a non-negative tensor with dtype `self.dtype`, have no\n",
      " |      fractional components, and such that\n",
      " |      `tf.reduce_sum(value, -1) = self.total_count`. Its shape must be broadcastable\n",
      " |      with `self.probs` and `self.total_count`.\n",
      " |      \n",
      " |      Args:\n",
      " |        value: `float` or `double` `Tensor`.\n",
      " |        name: The name to give this op.\n",
      " |      \n",
      " |      Returns:\n",
      " |        prob: a `Tensor` of shape `sample_shape(x) + self.batch_shape` with\n",
      " |          values of type `self.dtype`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from tensorflow.contrib.distributions.python.ops.multinomial.Multinomial:\n",
      " |  \n",
      " |  logits\n",
      " |      Vector of coordinatewise logits.\n",
      " |  \n",
      " |  probs\n",
      " |      Probability of of drawing a `1` in that coordinate.\n",
      " |  \n",
      " |  total_count\n",
      " |      Number of trials used to construct a sample.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from tensorflow.contrib.distributions.python.ops.distribution.Distribution:\n",
      " |  \n",
      " |  batch_shape_tensor(self, name='batch_shape_tensor')\n",
      " |      Shape of a single sample from a single event index as a 1-D `Tensor`.\n",
      " |      \n",
      " |      The batch dimensions are indexes into independent, non-identical\n",
      " |      parameterizations of this distribution.\n",
      " |      \n",
      " |      Args:\n",
      " |        name: name to give to the op\n",
      " |      \n",
      " |      Returns:\n",
      " |        batch_shape: `Tensor`.\n",
      " |  \n",
      " |  cdf(self, value, name='cdf')\n",
      " |      Cumulative distribution function.\n",
      " |      \n",
      " |      Given random variable `X`, the cumulative distribution function `cdf` is:\n",
      " |      \n",
      " |      ```none\n",
      " |      cdf(x) := P[X <= x]\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |        value: `float` or `double` `Tensor`.\n",
      " |        name: The name to give this op.\n",
      " |      \n",
      " |      Returns:\n",
      " |        cdf: a `Tensor` of shape `sample_shape(x) + self.batch_shape` with\n",
      " |          values of type `self.dtype`.\n",
      " |  \n",
      " |  copy(self, **override_parameters_kwargs)\n",
      " |      Creates a deep copy of the distribution.\n",
      " |      \n",
      " |      Note: the copy distribution may continue to depend on the original\n",
      " |      initialization arguments.\n",
      " |      \n",
      " |      Args:\n",
      " |        **override_parameters_kwargs: String/value dictionary of initialization\n",
      " |          arguments to override with new values.\n",
      " |      \n",
      " |      Returns:\n",
      " |        distribution: A new instance of `type(self)` initialized from the union\n",
      " |          of self.parameters and override_parameters_kwargs, i.e.,\n",
      " |          `dict(self.parameters, **override_parameters_kwargs)`.\n",
      " |  \n",
      " |  covariance(self, name='covariance')\n",
      " |      Covariance.\n",
      " |      \n",
      " |      Covariance is (possibly) defined only for non-scalar-event distributions.\n",
      " |      \n",
      " |      For example, for a length-`k`, vector-valued distribution, it is calculated\n",
      " |      as,\n",
      " |      \n",
      " |      ```none\n",
      " |      Cov[i, j] = Covariance(X_i, X_j) = E[(X_i - E[X_i]) (X_j - E[X_j])]\n",
      " |      ```\n",
      " |      \n",
      " |      where `Cov` is a (batch of) `k x k` matrix, `0 <= (i, j) < k`, and `E`\n",
      " |      denotes expectation.\n",
      " |      \n",
      " |      Alternatively, for non-vector, multivariate distributions (e.g.,\n",
      " |      matrix-valued, Wishart), `Covariance` shall return a (batch of) matrices\n",
      " |      under some vectorization of the events, i.e.,\n",
      " |      \n",
      " |      ```none\n",
      " |      Cov[i, j] = Covariance(Vec(X)_i, Vec(X)_j) = [as above]\n",
      " |      ```\n",
      " |      \n",
      " |      where `Cov` is a (batch of) `k' x k'` matrices,\n",
      " |      `0 <= (i, j) < k' = reduce_prod(event_shape)`, and `Vec` is some function\n",
      " |      mapping indices of this distribution's event dimensions to indices of a\n",
      " |      length-`k'` vector.\n",
      " |      \n",
      " |      Args:\n",
      " |        name: The name to give this op.\n",
      " |      \n",
      " |      Returns:\n",
      " |        covariance: Floating-point `Tensor` with shape `[B1, ..., Bn, k', k']`\n",
      " |          where the first `n` dimensions are batch coordinates and\n",
      " |          `k' = reduce_prod(self.event_shape)`.\n",
      " |  \n",
      " |  entropy(self, name='entropy')\n",
      " |      Shannon entropy in nats.\n",
      " |  \n",
      " |  event_shape_tensor(self, name='event_shape_tensor')\n",
      " |      Shape of a single sample from a single batch as a 1-D int32 `Tensor`.\n",
      " |      \n",
      " |      Args:\n",
      " |        name: name to give to the op\n",
      " |      \n",
      " |      Returns:\n",
      " |        event_shape: `Tensor`.\n",
      " |  \n",
      " |  is_scalar_batch(self, name='is_scalar_batch')\n",
      " |      Indicates that `batch_shape == []`.\n",
      " |      \n",
      " |      Args:\n",
      " |        name: The name to give this op.\n",
      " |      \n",
      " |      Returns:\n",
      " |        is_scalar_batch: `bool` scalar `Tensor`.\n",
      " |  \n",
      " |  is_scalar_event(self, name='is_scalar_event')\n",
      " |      Indicates that `event_shape == []`.\n",
      " |      \n",
      " |      Args:\n",
      " |        name: The name to give this op.\n",
      " |      \n",
      " |      Returns:\n",
      " |        is_scalar_event: `bool` scalar `Tensor`.\n",
      " |  \n",
      " |  log_cdf(self, value, name='log_cdf')\n",
      " |      Log cumulative distribution function.\n",
      " |      \n",
      " |      Given random variable `X`, the cumulative distribution function `cdf` is:\n",
      " |      \n",
      " |      ```none\n",
      " |      log_cdf(x) := Log[ P[X <= x] ]\n",
      " |      ```\n",
      " |      \n",
      " |      Often, a numerical approximation can be used for `log_cdf(x)` that yields\n",
      " |      a more accurate answer than simply taking the logarithm of the `cdf` when\n",
      " |      `x << -1`.\n",
      " |      \n",
      " |      Args:\n",
      " |        value: `float` or `double` `Tensor`.\n",
      " |        name: The name to give this op.\n",
      " |      \n",
      " |      Returns:\n",
      " |        logcdf: a `Tensor` of shape `sample_shape(x) + self.batch_shape` with\n",
      " |          values of type `self.dtype`.\n",
      " |  \n",
      " |  log_survival_function(self, value, name='log_survival_function')\n",
      " |      Log survival function.\n",
      " |      \n",
      " |      Given random variable `X`, the survival function is defined:\n",
      " |      \n",
      " |      ```none\n",
      " |      log_survival_function(x) = Log[ P[X > x] ]\n",
      " |                               = Log[ 1 - P[X <= x] ]\n",
      " |                               = Log[ 1 - cdf(x) ]\n",
      " |      ```\n",
      " |      \n",
      " |      Typically, different numerical approximations can be used for the log\n",
      " |      survival function, which are more accurate than `1 - cdf(x)` when `x >> 1`.\n",
      " |      \n",
      " |      Args:\n",
      " |        value: `float` or `double` `Tensor`.\n",
      " |        name: The name to give this op.\n",
      " |      \n",
      " |      Returns:\n",
      " |        `Tensor` of shape `sample_shape(x) + self.batch_shape` with values of type\n",
      " |          `self.dtype`.\n",
      " |  \n",
      " |  mean(self, name='mean')\n",
      " |      Mean.\n",
      " |  \n",
      " |  mode(self, name='mode')\n",
      " |      Mode.\n",
      " |  \n",
      " |  sample(self, sample_shape=(), seed=None, name='sample')\n",
      " |      Generate samples of the specified shape.\n",
      " |      \n",
      " |      Note that a call to `sample()` without arguments will generate a single\n",
      " |      sample.\n",
      " |      \n",
      " |      Args:\n",
      " |        sample_shape: 0D or 1D `int32` `Tensor`. Shape of the generated samples.\n",
      " |        seed: Python integer seed for RNG\n",
      " |        name: name to give to the op.\n",
      " |      \n",
      " |      Returns:\n",
      " |        samples: a `Tensor` with prepended dimensions `sample_shape`.\n",
      " |  \n",
      " |  stddev(self, name='stddev')\n",
      " |      Standard deviation.\n",
      " |      \n",
      " |      Standard deviation is defined as,\n",
      " |      \n",
      " |      ```none\n",
      " |      stddev = E[(X - E[X])**2]**0.5\n",
      " |      ```\n",
      " |      \n",
      " |      where `X` is the random variable associated with this distribution, `E`\n",
      " |      denotes expectation, and `stddev.shape = batch_shape + event_shape`.\n",
      " |      \n",
      " |      Args:\n",
      " |        name: The name to give this op.\n",
      " |      \n",
      " |      Returns:\n",
      " |        stddev: Floating-point `Tensor` with shape identical to\n",
      " |          `batch_shape + event_shape`, i.e., the same shape as `self.mean()`.\n",
      " |  \n",
      " |  survival_function(self, value, name='survival_function')\n",
      " |      Survival function.\n",
      " |      \n",
      " |      Given random variable `X`, the survival function is defined:\n",
      " |      \n",
      " |      ```none\n",
      " |      survival_function(x) = P[X > x]\n",
      " |                           = 1 - P[X <= x]\n",
      " |                           = 1 - cdf(x).\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |        value: `float` or `double` `Tensor`.\n",
      " |        name: The name to give this op.\n",
      " |      \n",
      " |      Returns:\n",
      " |        `Tensor` of shape `sample_shape(x) + self.batch_shape` with values of type\n",
      " |          `self.dtype`.\n",
      " |  \n",
      " |  variance(self, name='variance')\n",
      " |      Variance.\n",
      " |      \n",
      " |      Variance is defined as,\n",
      " |      \n",
      " |      ```none\n",
      " |      Var = E[(X - E[X])**2]\n",
      " |      ```\n",
      " |      \n",
      " |      where `X` is the random variable associated with this distribution, `E`\n",
      " |      denotes expectation, and `Var.shape = batch_shape + event_shape`.\n",
      " |      \n",
      " |      Args:\n",
      " |        name: The name to give this op.\n",
      " |      \n",
      " |      Returns:\n",
      " |        variance: Floating-point `Tensor` with shape identical to\n",
      " |          `batch_shape + event_shape`, i.e., the same shape as `self.mean()`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from tensorflow.contrib.distributions.python.ops.distribution.Distribution:\n",
      " |  \n",
      " |  param_shapes(sample_shape, name='DistributionParamShapes') from tensorflow.contrib.distributions.python.ops.distribution._DistributionMeta\n",
      " |      Shapes of parameters given the desired shape of a call to `sample()`.\n",
      " |      \n",
      " |      This is a class method that describes what key/value arguments are required\n",
      " |      to instantiate the given `Distribution` so that a particular shape is\n",
      " |      returned for that instance's call to `sample()`.\n",
      " |      \n",
      " |      Subclasses should override class method `_param_shapes`.\n",
      " |      \n",
      " |      Args:\n",
      " |        sample_shape: `Tensor` or python list/tuple. Desired shape of a call to\n",
      " |          `sample()`.\n",
      " |        name: name to prepend ops with.\n",
      " |      \n",
      " |      Returns:\n",
      " |        `dict` of parameter name to `Tensor` shapes.\n",
      " |  \n",
      " |  param_static_shapes(sample_shape) from tensorflow.contrib.distributions.python.ops.distribution._DistributionMeta\n",
      " |      param_shapes with static (i.e. `TensorShape`) shapes.\n",
      " |      \n",
      " |      This is a class method that describes what key/value arguments are required\n",
      " |      to instantiate the given `Distribution` so that a particular shape is\n",
      " |      returned for that instance's call to `sample()`. Assumes that the sample's\n",
      " |      shape is known statically.\n",
      " |      \n",
      " |      Subclasses should override class method `_param_shapes` to return\n",
      " |      constant-valued tensors when constant values are fed.\n",
      " |      \n",
      " |      Args:\n",
      " |        sample_shape: `TensorShape` or python list/tuple. Desired shape of a call\n",
      " |          to `sample()`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        `dict` of parameter name to `TensorShape`.\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: if `sample_shape` is a `TensorShape` and is not fully defined.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from tensorflow.contrib.distributions.python.ops.distribution.Distribution:\n",
      " |  \n",
      " |  allow_nan_stats\n",
      " |      Python `bool` describing behavior when a stat is undefined.\n",
      " |      \n",
      " |      Stats return +/- infinity when it makes sense. E.g., the variance of a\n",
      " |      Cauchy distribution is infinity. However, sometimes the statistic is\n",
      " |      undefined, e.g., if a distribution's pdf does not achieve a maximum within\n",
      " |      the support of the distribution, the mode is undefined. If the mean is\n",
      " |      undefined, then by definition the variance is undefined. E.g. the mean for\n",
      " |      Student's T for df = 1 is undefined (no clear way to say it is either + or -\n",
      " |      infinity), so the variance = E[(X - mean)**2] is also undefined.\n",
      " |      \n",
      " |      Returns:\n",
      " |        allow_nan_stats: Python `bool`.\n",
      " |  \n",
      " |  batch_shape\n",
      " |      Shape of a single sample from a single event index as a `TensorShape`.\n",
      " |      \n",
      " |      May be partially defined or unknown.\n",
      " |      \n",
      " |      The batch dimensions are indexes into independent, non-identical\n",
      " |      parameterizations of this distribution.\n",
      " |      \n",
      " |      Returns:\n",
      " |        batch_shape: `TensorShape`, possibly unknown.\n",
      " |  \n",
      " |  dtype\n",
      " |      The `DType` of `Tensor`s handled by this `Distribution`.\n",
      " |  \n",
      " |  event_shape\n",
      " |      Shape of a single sample from a single batch as a `TensorShape`.\n",
      " |      \n",
      " |      May be partially defined or unknown.\n",
      " |      \n",
      " |      Returns:\n",
      " |        event_shape: `TensorShape`, possibly unknown.\n",
      " |  \n",
      " |  name\n",
      " |      Name prepended to all ops created by this `Distribution`.\n",
      " |  \n",
      " |  parameters\n",
      " |      Dictionary of parameters used to instantiate this `Distribution`.\n",
      " |  \n",
      " |  reparameterization_type\n",
      " |      Describes how samples from the distribution are reparameterized.\n",
      " |      \n",
      " |      Currently this is one of the static instances\n",
      " |      `distributions.FULLY_REPARAMETERIZED`\n",
      " |      or `distributions.NOT_REPARAMETERIZED`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        An instance of `ReparameterizationType`.\n",
      " |  \n",
      " |  validate_args\n",
      " |      Python `bool` indicating possibly expensive checks are enabled.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(Multinomial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
